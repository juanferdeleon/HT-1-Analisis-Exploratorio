---
title: "HT 3. Árboles de decisión"
author: "Juan De Leon, Maria Jose Castro, Jose Block"
date: "2/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r include=FALSE}
df = read.csv("./train.csv")
```

## - Analisis Exploratorio

* Clasificacion de Variables: 

Aqui se pueden apreciar las variables con las que cuenta este set de datos. Se analizó que significaba cada variable y asi se pudo clasificar cada una de ellas.

```{r echo=FALSE }

data.frame("Variable"=c(colnames(df)), "Tipo de Variable"=c("N/A", "Cualitativa Nominal", "Cualitativa Nominal", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Ordinal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cualitativa Nominal", "Cuantitativa Discreta", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Nominal", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cuantitativa Discreta", "Cualitativa Ordinal", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cualitativa Nominal", "Cualitativa Ordinal", "Cualitativa Nominal", "Cualitativa Nominal", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cualitativa Ordinal", "Cuantitativa Discreta", "Cualitativa Nominal", "Cuantitativa Discreta", "Cualitativa Ordinal", "Cualitativa Nominal", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Nominal", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cuantitativa Discreta", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Nominal", "Cuantitativa Discreta", "Cualitativa Ordinal", "Cualitativa Ordinal", "Cualitativa Nominal", "Cuantitativa Discreta"))

```

Histogramas

```{r echo=FALSE}

library(tidyverse)

ggplot(data=df) + geom_histogram(mapping = aes(x = LotArea))
ggplot(data=df) + geom_histogram(mapping = aes(x = YearBuilt))
ggplot(data=df) + geom_histogram(mapping = aes(x = YrSold))
ggplot(data=df) + geom_histogram(mapping = aes(x = SalePrice))


```

Graficas de Puntos

```{r echo=FALSE}

ggplot(data=df) + geom_point(mapping = aes(y = SalePrice, x = LotArea)) 
ggplot(data=df) + geom_point(mapping = aes(y = SalePrice, x = YearBuilt))
ggplot(data=df) + geom_point(mapping = aes(y = SalePrice, x = YrSold))
q<-ggplot(data=df) + geom_point(mapping = aes(y = SalePrice, x = Neighborhood))
q + theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))
ggplot(data=df) + geom_point(mapping = aes(y = SalePrice, x = GarageType))
ggplot(data=df) + geom_point(mapping = aes(y = SalePrice, x = GarageArea))


```
Un breve resumen:
```{r echo=FALSE}
library(tidyverse)
sd<-df %>% select(2:81)
library(gtsummary)
tbl_summary(sd)
```


Se escogieron las variables representadas en las graficas presentadas anteriormente ya que se considero que estas estan fuertemente relacionadas al Sale Price. Precisamente se obtuvo mucha informacion a partir de esto. Las variables cuantitativas, en su mayoria, tienen una distribucion normal. Asimismo, se pudo identificar cuales eran los rangos de precios y que factores influian mas en el mismo.


## - Analisis de Grupos

```{r echo=FALSE, include=FALSE warning=FALSE}
set.seed(20)
data <- df[,c("LotFrontage","LotArea","YearBuilt","GarageYrBlt","GarageArea","YrSold", "SalePrice")]
data <- na.omit(data)

wssplot <- function(test1, nc=20, seed=123){
  wss <- (nrow(data)-1)*sum(apply(data,2,var))
  for (i in 2:10) 
    wss[i] <- sum(kmeans(data, centers=i)$withinss)
  plot(1:10, wss, type="b", xlab="Number of Clusters",  ylab="Within groups sum of squares")
}

```

Para realizar la grafica de codo se seleccionaron las variables cuantitativas que consideramos mas relevantes que son factores que podrian influir para deterinar el precio de venta de una casa. Mediante esta grafica de codo podemos determinar cuantos clusters seran pertinentes para nuestro analisis de datos. 

```{r echo=FALSE, warning=FALSE}
wssplot(test1, nc=20)
```

El grafico antes mostrado nos indica que podemos hacer una eleccion entre 3 y 5 clusters por lo que se decidira utilizar 4 clusters para el analisis de datos.

```{r echo=FALSE, warning=FALSE}

library(tidyverse)  # data manipulation
library(cluster)    # clustering algorithms
library(factoextra)

k1 <- kmeans(data, centers = 3, nstart = 25)
fviz_cluster(k1, data = data)

```

Luego del agrupamiento se obtuvo la grafica anterior, sin embargo, dada la cantidad de datos y la cercania entre valores no se puede apreciar con certeza los clusters generados. Por que que decidimos separar los clusters por variables especificas. Por ejemplo, se decidio ver los clusters creados al comparar unicamente el año en que se realizó la casa y su precio de venta.

```{r echo=FALSE, warning=FALSE}
mydata <- data %>% select(3, 7)
a<- data.matrix(mydata)
km<-kmeans(a, 3, iter.max = 10, nstart = 1)
plot(a, col = km$cluster)
points(km$centers, col = 1:5, type= "p",pch = 8, cex = 10)
```

De esta manera se pudo apreciar mejor la division entre grupoos. Y se realizo asi con todas las variables que considermaos necesarias para el analisis.

```{r echo=FALSE, warning=FALSE}

mydata <- data %>% select(1, 7)
a<- data.matrix(mydata)
km<-kmeans(a, 3, iter.max = 10, nstart = 1)
plot(a, col = km$cluster)
points(km$centers, col = 1:5, type= "p",pch = 8, cex = 10)

mydata <- data %>% select(2, 7)
a<- data.matrix(mydata)
km<-kmeans(a, 4, iter.max = 10, nstart = 1)
plot(a, col = km$cluster)
points(km$centers, col = 1:5, type= "p",pch = 8, cex = 10)

mydata <- data %>% select(4, 7)
a<- data.matrix(mydata)
km<-kmeans(a, 3, iter.max = 10, nstart = 1)
plot(a, col = km$cluster)
points(km$centers, col = 1:5, type= "p",pch = 8, cex = 10)

mydata <- data %>% select(5, 7)
a<- data.matrix(mydata)
km<-kmeans(a, 3, iter.max = 10, nstart = 1)
plot(a, col = km$cluster)
points(km$centers, col = 1:5, type= "p",pch = 8, cex = 10)

mydata <- data %>% select(6, 7)
a<- data.matrix(mydata)
km<-kmeans(a, 3, iter.max = 10, nstart = 1)
plot(a, col = km$cluster)
points(km$centers, col = 1:5, type= "p",pch = 8, cex = 10)
```


Percentiles:

```{r echo=FALSE}
quantile(df$SalePrice, c(.33, .66, .99))
```
## - Clasificacion de Casas (Economicas, Intermedias y Caras)

En base al analisis exploratorio realizado se separaron las variables que consideramos aportan mayor valor a nuestro analisis sobre los factores que determinan el precio de venta. Estas mismas se utilizaron para determinar los clusters. De esta manera se eligieron los 4 grupos. 

A partir de los clusters obtenidos y los percentiles se determino el rango de precio para cada grupo tomando en cuenta lusters y percentiles y aproximando entre sí. 

1. Casas Económicas: 0.00 a 145,000
2. Casas Intermedias:145,000 a 205,000
3. Casas Caras: 205,000 a 410,000
4. Casas Muy caras:

```{r echo=FALSE, warning=FALSE}
library(plyr)
df$tipoDeCasa = cut(df$SalePrice,c(0,145000,205000,410000), labels = c("economico", "intermedio", "caro"))

```
## - Dividir Set de Datos (Entrenamiento y Prueba)

Se utilizara el 70% de los datos para entrenamiento y el 30% restante para prueba. Es necesario brindar una cantidad sustancial de datos al algoritmo para entrenarlo y asi realizar un buen analisis y que el algoritmo pueda clasificar de manera correcta el set de datos. Se eligieron registros al azar para hacer la clasificacion.


## - Arbol de Clasificacion

```{r echo=FALSE, warning=FALSE}

library(rpart)
library(caret)
library(tree)
library(rpart.plot)
library(randomForest)

datos <- df

# variable respuesta la clase de la flor
porciento <- 70/100



datosFTree <- subset (datos, select = -c(1,15,23,24,32,43,81))



trainRowsNumber<-sample(1:nrow(datosFTree),porciento*nrow(datosFTree))
train<-datosFTree[trainRowsNumber,]
test<-datosFTree[-trainRowsNumber,]
arbolModelo<-rpart(tipoDeCasa~.,datosFTree,method = "class")
rpart.plot(arbolModelo)

save(train,test,arbolModelo, file = "Variables.RData")
load("Variables.RData")
dt_model<-rpart(tipoDeCasa~.,train,method = "class")
rpart.plot(dt_model)

head(test)
prediccion <- predict(dt_model, newdata = test)

columnaMasAlta<-apply(prediccion, 1, function(x) colnames(prediccion)[which.max(x)])
test$prediccion<-columnaMasAlta

cfm<-confusionMatrix(as.factor(test$prediccion),test$tipoDeCasa)
cfm
```

## - Arbol de Regresion


```{r echo=FALSE, warning=FALSE}

percent <- 70/100
cluster <- data
km<-kmeans(data,3)
data$grupo<-km$cluster

datosFTree <- subset (datos, select = -c(1,15,23,24,32,43,81))


set.seed(321)

trainRowsNumber<-sample(1:nrow(datosFTree),percent*nrow(datosFTree))

train<-datosFTree[trainRowsNumber,]
test<-datosFTree[-trainRowsNumber,]

dt_model<-rpart(tipoDeCasa~.,train,method = "anova")
rpart.plot(dt_model)


```

## -Random Forest
```{r}
library(caret)
#trainData<-df[trainRowsNumber,]
#test<-df[-trainRowsNumber,]
#ct<-trainControl(method = "cv",trainData,number=10, verboseIter=T)
#model_random_forest<-train(tipoDeCasa~.,data=trainData,method="rf",trControl = ct)
#prediccionrfVC<-predict(model_random_forest,newdata = test)
#test$predrfVC<-prediccionrfVC

#cfmCaret <- confusionMatrix(table(test$predrfVC,test$tipoDeCasa))
#cfmCaret

```

## - Analisis 


La primer variable que toma el arbol de regresion es el año en que se construyó la casa, el algoritmo nos indica que el 57% de las casas fue construida antes del año 1984 y el 43% restante fue construido en 1984 o despues. Seguido de esto, con las casas construidas antes del año 1984(57%), el arbol separa por el frente de la casa, en donde el 40% es menor a 78 ft. y el 17% es mayor o igual que 78 ft. ambos grupos de casas entran en el grupo 2 (casas intermedias). Ahora bien, del otro lado del arbol, con las casas construidas despues de 1983(43%) el arbol divide por el area del garage, donde el 11% es menor que 705ft cubicos y el 31% es mayor o igual a 705ft cubicos. Las casas con un garage menor a 705ft cubicos(11%) se divide en el tamaño del terreno, en donde el 6% es menor que 1200 ft cubicos y el 5% es mayor o igual. El primer grupo entra dentro de las casas economicas y el segundo en las casas intermedias. Seguido de esto, las casas con un garage mayor o igual a 705ft. cubicos(31%)se divide en las casas con un terreno menor a 3706ft. cubicos(3%) y casas con un terreno mayor o igual a 3706ft. (29%). Las casas con un area menor a 3706ft cubicos entra dentro del grupo de casas intermedias, sin embargo, las casas con un area mayor de terreno(29%) se dividen de la siguiente manera: Frente de la casa mayor o igual a 101 ft. el 2% tienen un frente menor y el 27% tienen un frente mayor. Independientemente de que division sea, ambas entran dentro del grupo de casas caras.

En el arbol de clasificacion, la raíz se basa en OverallQual comparando con 7 y se separa entre economico e intermedio y caro e intermiedio. Los siguientes nodos son GarageCars y GrLivArea, seguido de GarageType, GarageYrBlt , Neighborhood y BldgType. Repartiendose un total de 45% economico, 30% intermedio y 25% caro. Luego se corrieron las pruebas para hacer predicciones y se compararon los resultados con una matriz de confusion que dio un indice de precision de 0.7361 con un rango de presición de (0.6919, 0.7771). La variable mas dificil de identificar es "intermedio"

Por ultimo para el Random Forest.  Para la muestra de datos seleccionada podemos ver que si es posible encontrar un parametro que nos brinde un random forest con resultados mas precisos. Estos resultados pueden variar dependiento del parametro seleccionado, en este caso fue el tipoDeCasa. En el caso de mostrar la matriz de confusion podemos ver que el numero de arboles generados no influye de forma significativa en el sesgo entre precios. Por el nivel de accuracy obtenido podemos determinar que el random forest fue exitoso con su proyeccion y queda abierto a utilizar el mimo medoto con otro parametro y evaluar posibles resulados no contemplados. El random forest resulto ser una herramienta capaz de detectar la importancia de las variables y otros parametros. Asi como tambien se demostro que su nivel de Accuracy depende de la seleccion de parametros, pues de ser una varibale poco significativa se veria un aumento en el sesgo de los datos y los resultados lanzados por el arbol.


