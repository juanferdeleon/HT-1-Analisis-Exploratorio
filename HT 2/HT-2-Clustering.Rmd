---
title: "HT-1-Analisis-Exploratorio"
author: "Juan De Leon, Maria Jose Castro, Jose Block"
date: "02/22/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Hoja de Trabajo 2
### Clustering


1. Haga el preprocesamiento del dataset, explique qué variables no aportan información a la generación de grupos y por qué. Describa con qué variables calculará los grupos.


```{r echo=FALSE}
variables <- matrix(c('Id', 'imdb_id', 'original_title', 'homepage', 'tagline', 'overview'), ncol=1, byrow=TRUE)
colnames(variables) <- c("Variables que no aportan inforacion para el clustering")
rownames(variables) <- c(1, 2, 3, 4, 5, 6)
variables <- as.table(variables)
variables
```

Se consideró que las variables mencionadas anteriormente no aportan ningun valor significatico en la formacion de grupos ya que estas son caracteristicas unicas de cada registro dentro del data set. 

```{r}

dataframe = read.csv("../Data/tmdb-movies.csv")
dataframe <- na.omit(dataframe)

dataframe$popularity = ifelse(is.na(dataframe$popularity),ave(dataframe$popularity, FUN = function(x) mean(x, na.rm = 'TRUE')),dataframe$popularity)
dataframe$budget = ifelse(is.na(dataframe$budget),ave(dataframe$budget, FUN = function(x) mean(x, na.rm = 'TRUE')),dataframe$budget)
dataframe$revenue = ifelse(is.na(dataframe$revenue),ave(dataframe$revenue, FUN = function(x) mean(x, na.rm = 'TRUE')),dataframe$revenue)
dataframe$runtime = ifelse(is.na(dataframe$runtime),ave(dataframe$runtime, FUN = function(x) mean(x, na.rm = 'TRUE')),dataframe$runtime)
dataframe$vote_count = ifelse(is.na(dataframe$vote_count),ave(dataframe$vote_count, FUN = function(x) mean(x, na.rm = 'TRUE')),dataframe$vote_count)
dataframe$vote_average = ifelse(is.na(dataframe$vote_average),ave(dataframe$vote_average, FUN = function(x) mean(x, na.rm = 'TRUE')),dataframe$vote_average)

```

Se realizo una verificacion de las variables cuantitativas para remover los valores N/A.


2. Determine cuál es el número de grupos a formar más adecuado para los datos que está trabajando. Haga una gráfica de codo y explique la razón de la elección de la cantidad de clústeres con la que trabajará.

```{r echo=FALSE, include=FALSE, warning=FALSE}

set.seed(20)
data.matrix(dataframe)
test1 <- scale(na.omit(data.matrix(dataframe)[-1]))

wssplot <- function(test1, nc=20, seed=123){
  wss <- (nrow(test1)-1)*sum(apply(test1, 2, var))
  for (i in 2:nc){
    set.seed(seed)
    wss[i] <- sum(kmeans(test1, centers = i)$withinss)
  }
  plot(1:nc, wss, type="b", xlab="Number of clusters", ylab="Whithin Groups Sum of squares")
}
```

```{r echo=FALSE, warning=FALSE}

wssplot(test1, nc=20)


```


Para seleccionar el valor óptimo de k, se escoje entonces ese punto en donde ya no se dejan de producir variaciones importantes del valor de WCSS al aumentar k. En este caso, vemos que esto se produce a partir de k >= 5, por lo que evaluaremos los resultados del agrupamiento, por ejemplo, con los valores de 7, 8 y 9 a fin de observar el comportamiento del modelo.


3. Utilice 3 algoritmos existentes para agrupamiento. Compare los resultados generados por cada uno.

## Hierarchical Clustering

```{r echo=FALSE, warning=FALSE}

dft <- data.frame(dataframe$budget, dataframe$revenue)

library(ggdendro)
library(ggplot2)

seeds_df_sc <- as.data.frame(scale(dft))

dist_mat <- dist(seeds_df_sc, method = 'euclidean')

hclust_avg <- hclust(dist_mat, method = 'average')

plot(hclust_avg)

agrupamientoJ <- hclust(dist(dataframe, method = 'euclidean'), method = 'ward.D')
clases_aj <- cutree(agrupamientoJ, k = 5)
dataframe$cluster <- clases_aj

ggplot() + geom_point(aes(x = budget, y = revenue, color = cluster), data = dataframe, size = 2) +
  scale_colour_gradientn(colours=rainbow(4)) +
  ggtitle('Clusters de Datos con k = 5 / Agrupamiento Jerárquico') + 
  xlab('X') + ylab('Y')


```

## K-means Clustering
```{r echo=FALSE, warning=FALSE}
library(tidyverse)
library(factoextra)

mydata <- dataframe %>% select(4, 5)
x<- data.matrix(mydata)
km<-kmeans(x, 5, iter.max = 10, nstart = 1)
plot(x, col = km$cluster)
points(km$centers, col = 1:5, type= "p",pch = 8, cex = 10)

```

## Mixture of Gaussians
```{r echo=FALSE, warning=FALSE}
library(cluster) #Para calcular la silueta
library(mclust) #mixtures of gaussians
#dataframe = read.csv("tmdb-movies.csv")
dataframe <- na.omit(dataframe)
mc<-Mclust(dataframe[,4:5],5)
plot(mc, what = "classification", main="MClust Classification", colours=rainbow(5) )
dataframe$mxGau<-mc$classification
g1MC<-dataframe[dataframe$mxGau==1,]
g2MC<-dataframe[dataframe$mxGau==2,]
g3MC<-dataframe[dataframe$mxGau==3,]
g4MC<-dataframe[dataframe$mxGau==4,]
g5MC<-dataframe[dataframe$mxGau==5,]

#silmg<-silhouette(mc$classification,dist(dataframe[,4:5]))
#mean(silmg[,3]) 

```



4. Determine la calidad del agrupamiento hecho por cada algoritmo con el método de la silueta. Discuta los resultados.

```{r echo=FALSE, warning=FALSE}

library(cluster)    # clustering algorithms

avg_sil <- function(k) {
  km.res <- kmeans(na.omit(dft), centers = k, nstart = 25)
  ss <- silhouette(km.res$cluster, dist(dft))
  mean(ss[, 3])
}

k.values <- 2:14

avg_sil_values <- sapply(k.values, avg_sil)

plot(k.values, avg_sil_values,
       type = "b", pch = 19, frame = FALSE, 
       xlab = "Number of clusters K",
       ylab = "Average Silhouettes")

```

En base a los resultados del metodo de la silueta consideramos que hubiese sido mejor agrupar en 4 clusters en lugar de 5. En la grafica de codo que obtuvimos se nos dificulto cual cantidad de clusters era la adecuada. Al comparar los resultados de los algoritmos consideramos que K-Means Clustering es una muy buena opcion para analizar el set de datos y agrupamiento de las variables que nuestro grupo decidio estudiar. Esto ya que separo de una mejor manera los datos y se identifico mas facilmente los distintos clusters. 


5. Interprete los grupos basado en el conocimiento que tiene de los datos. Recuerde investigar las medidas de tendencia central de las variables continuas y las tablas de frecuencia de las variables categóricas pertenecientes a cada grupo. Identifique hallazgos interesantes debido a las agrupaciones y describa para qué le podría servir.

Podemos ver que dentro de cada una de las agrupaciones o clusters se tiene aproximadamente los mismos resultados en las graficas. El algoritmo que mostro una distribucion mucho mas centrada de 3 fue K-Means, ya que al compararla con Hierarchial podemos ver que varios de los valores del cluster 1, se separan en 2 dentro de K-Means, esto nos puede ayudar a que en el analisis se puedan dejar de tomar en cuenta los clusters de datos atipicos o extremos, sin perder gran candidad de informacion. O en caso contrario poder evaluar unicamente el cluster con informacion de los extremos.

```{r}
  summary(clases_aj)
  summary(x)
```


Ahora tomando como punto de partida Mixture of Gaussians podemos ver que es muy similar al Hierarchial Clustering. En el Hierarchial podemos ver que el agrupamiento fue seleccionado mas sobre el budjet, en este caso los clusters se pueden extienten a lo largo del eje X. Versus Mixture of Gaussians podemos ver que los primeros 2 clusters (en colores naranja y purpura) son muy similares con los de Hierarchial, pero al avanzar dentro del eje Y (revenue) se puede ver que se tienen 3 clusters distintos que acoplan los puntos de mayor contentracion. 

En este caso consideramos que el algoritmo a utilizar depende mucho de la data o informacion a evaluar, de igual manera el enfoque que se le desea dar al estudio. 

```{r}
   summary(x)
    summary(mc)
```

## 6. Trabajo Siguiente

```{r}
summary(dataframe)
```

En base al inciso anterior y el resumen que se hizo previamente podemos ver que se tiene mayor informacion con el algoritmo K-means, se utilizara este para poder hacer clustering sobre aproximadamente 4 grupos que fue lo recomendado, de igual manera despues de hacer este analisis se puede utilizar este algoritmo para comparar el revenue-popularity, para poder determinar si las peliculas mas populares son las que obtuvieron mayores ganancias o si el indice de popualaridad realmente es importante.
